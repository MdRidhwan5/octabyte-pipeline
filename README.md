# Octa Byte AI â€“ Logistics & Supplyâ€‘Chain Data Pipeline

**Author:** Md Ridhwan  
**Stack:** Python 3 Â· BeautifulSoup Â· Selenium Â· boto3 Â· NewsAPI Â· AWS Lambda Â· S3 Â· CloudWatch  

---

## 1. Project Goal

> Crawl six target logistics/supplyâ€‘chain web sources + a free news API, push raw + processed JSON to S3, and run the job automatically every day.

---

## 2. High-Level Flow

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Crawlers â”‚ â”€â†’ â”‚  Raw JSON in S3     â”‚ â”€â†’ â”‚  (Option)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    ETL     â”‚
        â†“                                               â†“
   AWS Lambda + EventBridge (scheduled)         CloudWatch Logs & Alarms (Errors â‰¥ 1)
````

---

## 3. Repo Layout

```text
.
â”œâ”€ crawlers/                    # One script per site
â”‚  â”œâ”€ gnosis.py
â”‚  â”œâ”€ logistics_of_logistics.py
â”‚  â””â”€ ...
â”œâ”€ lambda_newsapi/             # Self-contained NewsAPI Lambda package
â”‚  â”œâ”€ lambda_function.py
â”‚  â””â”€ requests/                # Vendored dependencies
â”œâ”€ upload_to_s3.py             # Helper for local development
â”œâ”€ requirements.txt
â””â”€ docs/
   â”œâ”€ architecture.png         # Exported from diagrams.net
   â””â”€ sample_outputs/
```

---

## 4. S3 Bucket Convention

```
s3://octabyte-data-pipeline/
â””â”€â”€ raw/
    â”œâ”€â”€ gnosisfreight/
    â”‚   â””â”€â”€ 2025-07-07/gnosis_2025-07-07.json
    â”œâ”€â”€ thelogisticsoflogistics/
    â””â”€â”€ ...
```

Each job writes to `raw/<source>/<YYYY-MM-DD>/`
Future processed data can be saved under a `processed/` prefix.

---

## 5. Running Locally

```bash
# Setup virtual environment
python -m venv venv
venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Run one crawler and upload result
python crawlers/gnosis.py
python upload_to_s3.py gnosis_2025-07-07.json gnosisfreight
```

ğŸ” Secrets are stored in a local `.env` file â€” this file is **never committed** to GitHub.

---

## 6. Automation on AWS

| Component                    | Purpose                                                         |
| ---------------------------- | --------------------------------------------------------------- |
| **Lambda `newsapi_crawler`** | Fetches news; zipped with `requests` inside                     |
| **EventBridge Rule**         | Triggers Lambda daily at 03:00 UTC                              |
| **IAM Role**                 | Minimal permissions: `s3:PutObject`, `logs:CreateLogGroup`, etc |
| **CloudWatch Alarm**         | Sends alert via email/SNS if `Errors â‰¥ 1`                       |

---

## 7. Extending â€“ Add a New Site in Under 3 Minutes

1. Add a new script inside `crawlers/` (e.g., `newsite.py`).
2. Make the script return data as a dict/list and save it as `YYYY-MM-DD.json`.
3. Run:
python upload_to_s3.py newfile.json newsource
4. (Optional) Add the script to a Lambda function or Airflow DAG.

---

## 8. Future Improvements & Roadmap

These features are suggested for future development:

* â± Parallel crawling via `Scrapy`, `asyncio`, or `ThreadPoolExecutor`.
* ğŸ” Text deduplication and keyword/keyphrase extraction using spaCy or KeyBERT.
* ğŸ³ Dockerized deployment using ECS Fargate or EKS.
* ğŸ“Š Athena/QuickSight integration for dashboards and queryable logs.

---

Â© 2025 Octa Byte AI. Internal use only.